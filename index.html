<!DOCTYPE html>

<html lang="en">
    <head>
        <meta charset="UTF-8">

        <meta name="description" content="Project website for EquivAct">
        <meta name="author" content="Congyue Deng">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="icon" href="icons/robot_icon.png">

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://kit.fontawesome.com/d91aa93e36.js" crossorigin="anonymous"></script>

        <title>EquivAct</title>
        <link rel="stylesheet" href="style.css">
    </head>

    <style>
        a {
            color: black;
        }

        a:hover {
            color: gray;
        }

        .body_text {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 400;
            line-height: 1.6rem;
            font-size: 1.04rem;
            color: rgba(0,0,0,0.8);
        }

        .grad_text {
            background: -webkit-linear-gradient(right, #003f5c, #58508d, #bc5090, #ff6361, #ffa600);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .text_title {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 700;
            line-height: 3.5rem;
            font-size: 3rem;
            padding-top: 3rem;
        }

        .text_venue {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 500;
            line-height: 2.0rem;
            font-size: 1.5rem;
        }

        .text_author {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 300;
            line-height: 1.2rem;
            font-size: 1.2rem;
            color: #666;
        }

        .text_resource {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 300;
            line-height: 1.25rem;
            font-size: 1.25rem;
            color: #666;
        }

        .text_header {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 700;
            line-height: 2.5rem;
            font-size: 1.3rem;
            margin-top: 0.4rem;
        }

        .figure_caption {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 400;
            line-height: 1.8rem;
            font-size: 1.0rem;
            color: rgb(77, 79, 83);
            display: block;
            width: 100%;
            text-align: center;
            padding-top: 0.3rem;
        }

        .text_reference {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            text-align: left;
            font-size: 0.7em;
            color: rgb(77, 79, 83);
        }

        .text_ack {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            text-align: center;
            font-size: 0.7em;
            font-weight: 700;
            color: rgba(0, 0, 0, 0.5);
        }

        .teaser_grid {
            display: grid;
            grid-template-columns: 33% 33% 33%;
            grid-template-rows: 1fr;
            justify-content: center;
            grid-auto-flow: row;
        }

        .teaser_fig {
            max-width: 99%;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .icons {
            height: 3.5em;
        }


        .big_figure {
            max-width: 100%;
            display: block;
            margin: 1em auto 1em;
        }

        .median_figure {
            max-width: 75%;
            display: block;
            margin: 1em auto 1em;
        }

        .color_switch {
            background-color: rgb(255, 255, 255);
        }

        .qual_sample {
            display: flex;
            width: 100%;
            flex-direction: row;
            flex-wrap: wrap;
            justify-content: center;
            gap: 0.5rem;
        }

        .table_title {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 700;
            font-size: 1.25rem;
            color: rgb(77, 79, 83);
            text-align: center;
        }

        .table_head {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 700;
            font-size: 1.1rem;
            color: rgb(77, 79, 83);
            text-align: center;
        }

        .table_elem {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
            font-weight: 400;
            font-size: 1.0rem;
            color: rgb(77, 79, 83);
            text-align: center;
            align-self: center;
        }

        .table_emph {
            font-weight: 700;
        }

    </style>

    <body>
    <section>
    <div class="container">
        <div class="row" id="title">
            <p class="col text-center mt-4 text_title">
                <span class="grad_text">EquivAct</span>: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation
            </p>
        </div>

        <div class="row" id="venue">
            <p class="col text-center text_venue">   
            </p>
        </div>

        
        <div class="row" id="authors">
            <div class="col-md-12 text-center mt-2 text_author">
                <ul class="list-inline">
                    <li class="list-inline-item mx-2">
                        <a href="https://yjy0625.github.io/">
                            <p>Jingyun Yang<sup>1</sup>*</p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-2">
                        <a href="https://cs.stanford.edu/~congyue/">
                            <p>Congyue Deng<sup>1</sup>*</p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-24">
                        <a href="https://jimmyyhwu.github.io/">
                            <p>Jimmy Wu<sup>2</sup></p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-24">
                        <a href="https://contactrika.github.io/">
                            <p>Rika Antonova<sup>1</sup></p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-2">
                        <a href="https://geometry.stanford.edu/member/guibas/">
                            <p>Leonidas Guibas<sup>1</sup></p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-2">
                        <a href="https://web.stanford.edu/~bohg/">
                            <p>Jeannette Bohg<sup>1</sup></p>
                        </a>
                    </li>
                    <br/>
                    <li class="list-inline-item mx-4">
                        <p>*Equal Contribution</p>
                    </li>
                    <li class="list-inline-item mx-4">
                        <p>Stanford University<sup>1</sup></p>
                    </li>
                    <li class="list-inline-item mx-4">
                        <p>Princeton University<sup>2</sup></p>
                    </li>
                </ul>
            </div>
        </div>
        

        <div class="row" id="resources">
            <div class="col text-center text_resource">
                <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                    </a>
                </span>
                <!-- <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                    </a>
                </span> -->
                <!-- Video Link. -->
                <span class="link-block">
                    <a href="https://youtu.be/NaPL2H1nnGU" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fa-brands fa-youtube"></i>
                        </span>
                        <span>Video</span>
                    </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fa-solid fa-code"></i></i>
                        </span>
                        <span>Code (Coming Soon)</span>
                    </a>
                </span>
            </div>
        </div>
    </div>
    </section>

    <div class="container">
    </div>

    <section>
        <div class="container">
        <div class="row" id="abstract">
            <div class="col my-3">
                <div class="row" style="position:relative">
                    <div class="grid-template-columns" style="width:75%;height:75%">
                        <img src="images/teaser.png" class="teaser_fig">
                    </div>
                    <div class="grid-template-columns" style="width:20%;height:20%">
                        <img src="images/results/real_exe.gif" class="teaser_fig" style="height:193px;position:absolute;bottom:0">
                    </div>
                </div>
                <br><br>

                <p class="text_header">
                    <span class="grad_text">Abstract</span>
                </p>

                <p class="body_text">
                    If a robot masters folding a kitchen towel, we would also expect it to master folding a beach towel. However, existing works for policy learning that rely on data set augmentations are still limited in achieving this level of generalization.
                    Our insight is to add equivariance into both the visual object representation and policy architecture.
                    We propose <strong>EquivAct</strong> which utilizes SIM(3)-equivariant network structures that guarantee generalization across all possible object translations, 3D rotations, and scales by construction. Training of EquivAct is done in two phases. We first pre-train a SIM(3)-equivariant visual representation on simulated scene point clouds. Then, we learn a SIM(3)-equivariant visuomotor policy on top of the pre-trained visual representation using a small amount of source task demonstrations.
                    We demonstrate that after training, the learned policy directly transfers to objects that substantially differ in scale, position and orientation from the source demonstrations.
                    In simulation, we evaluate our method in three manipulation tasks involving deformable and articulated objects thereby going beyond the typical rigid object manipulation tasks that prior works considered. We show that our method outperforms prior works that do not use equivariant architectures or do not use our contrastive pre-training procedure. We also show qualitative experiments on three real robot tasks, where the robot watches twenty demonstrations of a tabletop task and transfers zero-shot to a mobile manipulation task in a much larger setup.
                </p>
            </div>
        </div>
        </div>
    </section>

    <section>
        <div class="container">
        <div class="row">
            <div class="col my-3">
                <p class="text_header">
                    <span class="grad_text">Video</span>
                </p>
                <div class="text-center mb-4">
                    <div style="position:relative;padding-top:10px;">
                        <video height="auto" width="100%" controls="" style="border: 1px solid #000;">
                            <source src="./video/equivact.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
        </div>
    </section>

    <section>
        <div class="container">
        <div class="row" id="abstract">
            <div class="col my-3">
                <p class="text_header">
                    <span class="grad_text">Method Overview</span>
                </p>
                
                <p class="body_text">
                    <strong>Our method is composed of two phases: a representation learning phase and a policy learning phase.</strong>
                    In the <strong>representation learning</strong> phase, the agent is given a set of simulated point clouds that are recorded from objects of the same category as the objects in the target task but with a randomized nonuniform scaling. While the proposed architecture is equivariant to uniform scaling, we need to augment the training data in this way to account for non-uniform scaling. Note that the simulated data is not a demonstration of the target task and does not need to include robot actions. With the simulated data, we train a SIM(3)-equivariant encoder-decoder architecture that takes the scene point cloud as input and outputs the global and local features of the input point cloud. We use a contrastive learning loss on paired point cloud inputs during training, so that local features for corresponding object parts of objects in similar poses can be pulled closer than non-corresponding parts.
                </p>
                <img src="images/rep_learning.png" class="teaser_fig" style="width:75%">
                <br><br>
                <p class="body_text">
                    In the <strong>policy learning</strong> phase, we assume access to a small set of demonstrated trajectories of the task. This could be in the form of human demonstrations or teleoperated robot trajectories. With the demonstration data, we train a closedloop policy that takes a partial point cloud of the scene as input, uses the pre-trained encoder from the representation learning phase to obtain the global and local features of the input point cloud, and then passes the features through a SIM(3)-equivariant action prediction network to predict endeffector movements.
                </p>
                <img src="images/policy_learning.png" class="teaser_fig" style="width:75%">
                <br><br>
            </div>
        </div>
        </div>
    </section>

    <section class="color_switch">
        <div class="container">
        <div class="row" id="teaser">
            <div class="col my-3">
                <p class="text_header">
                    <span class="grad_text">Results</span>
                </p>

                <p class="body_text">
                    We evaluate our method on three robot manipulation tasks involving deformable and articulated objects: cloth folding, object covering, and box closing. In simulation, we show that our method performs better than baselines that do not use equivariant architectures and rely on augmentations to achieve generalization to unseen translations, rotations, and scales.
                </p>
                <img src="images/sim_results.jpg" class="teaser_fig" style="width:100%;margin-bottom:1rem">

                <p class="body_text">
                    Below, we show qualitative samples of simulated demonstrations and learned sim policies executed on an object with different size and pose from the object seen in the demos.
                </p>

                <div class="row qual_wrapper">
                    <div class="qual_sample">
                        <img src="images/results/fold_demo_cropped.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/cover_demo_cropped.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/close_demo_cropped.gif" class="qual_fig" style="height:200px">
                    </div>
                    <p class="figure_caption">Simulated Demonstrations</b></p>
                    <div class="qual_sample">
                        <img src="images/results/fold_ours_cropped.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/cover_ours_cropped.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/close_ours_cropped.gif" class="qual_fig" style="height:200px">
                    </div>
                    <p class="figure_caption">Simulated Target Task Executions</b></p>
                </div>

                <p class="body_text">
                    We also run real robot experiments on the same three tasks as the sim experiments. We show that our method performs significantly better than the best baseline in simulated experiments in all three tasks. Below, we show qualitative samples of real robot demos and real robot executions.
                </p>

                <div class="row qual_wrapper">
                    <div class="qual_sample">
                        <img src="images/results/real_demos_fold_4x_slower.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/real_demos_cover_4x_slower.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/real_demos_close_4x_slower.gif" class="qual_fig" style="height:200px">
                    </div>
                    <p class="figure_caption">Real Robot Demonstrations</b></p>
                    <div class="qual_sample">
                        <img src="images/results/real_folding_beige_towel-short.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/real_covering_red_blanket-short.gif" class="qual_fig" style="height:200px">
                        <img src="images/results/real_boxclosing_large_box_rot-short.gif" class="qual_fig" style="height:200px">
                    </div>
                    <p class="figure_caption">Real Robot Target Task Executions</b></p>
                </div>
            </div>
        </div>
        </div>
    </section>

        <div class="row" id="ack" style="margin-top: 1rem">
            <p class="col text_ack my-3">
                If you have any questions, please contact Jingyun Yang, Congyue Deng, and Rika Antonova ({jingyuny, congyue, rika.antonova} "at" stanford "dot" edu).
            </p>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>

    </body>
</html>